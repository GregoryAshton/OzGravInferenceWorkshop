{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division, print_function\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm, uniform\n",
    "\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to sampling\n",
    "\n",
    "In this notebook, we'll give a short introduction to sampling from a posterior probability distribution. \n",
    "\n",
    "This problem is quite straight forward and could be solved by just evaluating the posterior on a grid, or even analytically. Nevertheless, its useful example for when things get more complicated and we can't do that!\n",
    "\n",
    "This example is loosely based on [this blog post of Thomas Weicki](http://twiecki.github.io/blog/2015/11/10/mcmc-sampling/)\n",
    "\n",
    "## Setting up the problem\n",
    "\n",
    "Consider a situation where we have data $\\mathbf{d} = \\{d_1, d_2, \\ldots, d_N\\}$. We bin it in a histogram and it looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 10  # Number of data points\n",
    "fig, ax = plt.subplots()\n",
    "data = np.random.normal(0, 1, N)  # Generate the fake data\n",
    "ax.hist(data)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us imagine that we have a model of how the data was generated which is that $d_i$, the data, is generated from a Normal distribution, with known standard deviation of 1, but *unknown* mean. The question we wish to ask is \n",
    "\n",
    "> **given the observed data, what are the relative probabilities of different values of the mean?**. \n",
    "\n",
    "Mathematically, this corresponds to find the posterior distribution $ P(\\mu| \\mathbf{d})$ where $\\mu$ is the mean:\n",
    "\n",
    "$$ P(\\mu| \\mathbf{d}) = \\frac{P(\\mathbf{d}| \\mu) P(\\mu)}{P(\\mathbf{d})} $$\n",
    "\n",
    "For now, we are only interested in the relative probabilities of the different $\\mu$ values so we can drop the normalization:\n",
    "\n",
    "$$ P(\\mu| \\mathbf{d}) \\propto P(\\mathbf{d}| \\mu) P(\\mu)$$\n",
    "\n",
    "Finally, we'll now make an assumption that all the data are independent. This means we can do the following\n",
    "\n",
    "$$ P(\\mathbf{d}| \\mu) = \\prod_{i=1}^N P(d_i | \\mu) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting the model into the story\n",
    "\n",
    "So far we have been quite abstract and it may not be clear yet how to evalue the posterior. However, we made one statement earlier that is crucial:\n",
    "\n",
    "> the data is generated from a Normal distribution\n",
    "\n",
    "This means that we can test how *likely* the $i$th data point is by plugging it into a standard Normal distribution:\n",
    "\n",
    "$$ P(d_i| \\mu) = \\mathrm{Normal}(d_i; \\mu) = \\frac{1}{\\sqrt{2\\pi}} \\mathrm{exp}\\left(\\frac{-(d_i - \\mu)^2}{2}\\right)$$\n",
    "\n",
    "*Note - there is no $\\sigma$ here as we set it to be $\\sigma=1$, this is known as a standard Normal distribution*\n",
    "\n",
    "Let's write a python function to evaluate our likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def likelihood(data, mu):\n",
    "    \"\"\" Likelihood of the data (a vector), given mu \"\"\"\n",
    "    likelihood_per_di = np.exp(-(data - mu)**2 / 2) / np.sqrt(2*np.pi)\n",
    "    return np.prod(likelihood_per_di)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Priors\n",
    "The final thing left need to evaluate the unormalised posterior is the prior. In this case, we'll use a Uniform prior over a suitable range:\n",
    "\n",
    "$$ P(\\mu) = \\mathrm{Uniform}(-10, 10) = \\begin{array}{cc}\n",
    "\\frac{1}{20} & \\mathrm{if} -10 \\leq \\mu < 10 \\\\\n",
    "0 & \\mathrm{otherwise} \\end{array}$$\n",
    "\n",
    "Let's write a python function to evaluate our prior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior(mu):\n",
    "    \"\"\" Prior for mu\"\"\"\n",
    "    if (-10 <= mu) and mu < 10:\n",
    "        return 1/20.\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the posterior: hill climbing\n",
    "\n",
    "We'll now describe what is meant by sampling. Firstly, in a simple \"hill climbing\" context (you'll see why it's called this in a moment). \n",
    "\n",
    "The basic idea is we are going to \"walk\" around the parameter space. First, let's pick somewhere to start."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_current = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll decide how to \"walk\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "proposal_std_dev = 0.1\n",
    "proposal = np.random.normal(0, proposal_std_dev)\n",
    "print(proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we propose a step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_proposal = mu_current + proposal\n",
    "print(mu_proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, we have two \"positions\" in $\\mu$-space. We need a way to decide between them. We'll do this by computing the unnormalised posterior probability of both"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_current = likelihood(data, mu_current)\n",
    "prior_current = prior(mu_current)\n",
    "p_current = likelihood_current * prior_current\n",
    "print(p_current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "likelihood_proposal = likelihood(data, mu_proposal)\n",
    "prior_proposal = prior(mu_proposal)\n",
    "p_proposal = likelihood_proposal * prior_proposal\n",
    "print(p_proposal)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay these are tiny numbers! But, we can take a ratio and, if the proposal is larger than the current step, that is \"up hill\", i.e. that value of $\\mu$ is more probable than the current one.\n",
    "\n",
    "We can repeat this process to build a very simply hill walking climber with just this logic. Let's have a look at what that does. So that you can play around with it elsewhere, we will put it into a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def hill_walker(mu_init, nsamples=100, proposal_std_dev=0.01):\n",
    "    mu_current = mu_init\n",
    "    mu_values = [mu_current]\n",
    "    for n in range(nsamples):\n",
    "        # Propose a new step and compute the probabilities\n",
    "        proposal = np.random.normal(0, proposal_std_dev)\n",
    "        mu_proposal = mu_current + proposal\n",
    "        likelihood_current = likelihood(data, mu_current)\n",
    "        prior_current = prior(mu_current)\n",
    "        p_current = likelihood_current * prior_current\n",
    "\n",
    "        likelihood_proposal = likelihood(data, mu_proposal)\n",
    "        prior_proposal = prior(mu_proposal)\n",
    "        p_proposal = likelihood_proposal * prior_proposal\n",
    "\n",
    "        # If the proposed step is better than the current, p_accept > 1 so we accept it\n",
    "        p_accept = p_proposal / p_current\n",
    "        if p_accept > 1:\n",
    "            mu_current = mu_proposal\n",
    "        mu_values.append(mu_current)\n",
    "        \n",
    "    return mu_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running it for 100 steps and plotting the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_values = hill_walker(mu_init=-1, nsamples=500)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mu_values)\n",
    "ax.set_xlabel('step')\n",
    "ax.set_ylabel('$\\mu$')\n",
    "plt.show()\n",
    "\n",
    "print(mu_values[-10:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, this is great. It's telling us that the top of the hill (or the peak of the posterior) is near to 0.1 or so. But, this isn't sampling from the posterior. Eventually the walker will find the peak (to within numerical accuracy) and no more proposals will be accepted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling from the posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To sample from the posterior, we need to sometimes take a step downwards. We can do this by changing one line of our hill walker from\n",
    "\n",
    "```python \n",
    "if p_accept > 1:\n",
    "```\n",
    "\n",
    "to \n",
    "```python \n",
    "if p_accept > np.random.uniform(0, 1):\n",
    "```\n",
    "\n",
    "#### What does this do?\n",
    "\n",
    "`np.random.uniform(0, 1)` returns a (psuedo) random number uniformly distributed between 0 and 1, for example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform(0, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results is that\n",
    "\n",
    "* if `p_accept > 1` then we will always accept the move (we found a better point to move to)\n",
    "* but, if `p_accept < 1` we will **sometimes** move there\n",
    "\n",
    "### The maths\n",
    "\n",
    "What is `p_accept` really? Well, if we call $\\mu_c$ the current value and $\\mu_p$ the proposed value\n",
    "\n",
    "$$ \\texttt{p_accept} = \n",
    "\\frac{P(\\mathbf{d} | \\mu_c) P(\\mu_c)}{P(\\mathbf{d} | \\mu_p) P(\\mu_p)} =\n",
    "\\frac{\n",
    "\\frac{P(\\mathbf{d} | \\mu_c) P(\\mu_c)}{P(\\mathbf{d})}\n",
    "}{\n",
    "\\frac{P(\\mathbf{d} | \\mu_p) P(\\mu_p)}{P(\\mathbf{d})}\n",
    "}\n",
    "=\\frac{P(\\mu_c| \\mathbf{d})}{P(\\mu_p| \\mathbf{d})}\n",
    "$$\n",
    "\n",
    "So `p_accept` is the ratio of probabilities at two different points (it's  convienient that the normalisation, which we often can't compute, happens to drop out).\n",
    "\n",
    "Let's implement this and see the difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sampler(mu_init, nsamples=100, proposal_std_dev=0.05):\n",
    "    mu_current = mu_init\n",
    "    mu_values = [mu_current]\n",
    "    \n",
    "    nacceptance = 0\n",
    "    for n in range(nsamples):\n",
    "        # Propose a new step and compute the probabilities\n",
    "        proposal = np.random.normal(0, proposal_std_dev)\n",
    "        mu_proposal = mu_current + proposal\n",
    "        likelihood_current = likelihood(data, mu_current)\n",
    "        prior_current = prior(mu_current)\n",
    "        p_current = likelihood_current * prior_current\n",
    "\n",
    "        likelihood_proposal = likelihood(data, mu_proposal)\n",
    "        prior_proposal = prior(mu_proposal)\n",
    "        p_proposal = likelihood_proposal * prior_proposal\n",
    "\n",
    "        # If the proposed step is better than the current, p_accept > 1 so we accept it\n",
    "        p_accept = p_proposal / p_current\n",
    "        if p_accept > np.random.uniform(0, 1):\n",
    "            mu_current = mu_proposal\n",
    "            nacceptance += 1  # This is just keeping track of how many times proposals where accepted.\n",
    "        mu_values.append(mu_current)\n",
    "        \n",
    "    print('Finished, acceptance ratio = {}'.format(nacceptance/nsamples))\n",
    "        \n",
    "    return mu_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_values = sampler(mu_init=-1, nsamples=100, proposal_std_dev=0.01)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mu_values)\n",
    "ax.set_xlabel('step')\n",
    "ax.set_ylabel('$\\mu$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay now that looks somewhat different! Let's see what happens when we get lots of samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_values = sampler(mu_init=-1, nsamples=10000, proposal_std_dev=0.01)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mu_values)\n",
    "ax.set_xlabel('step')\n",
    "ax.set_ylabel('$\\mu$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "and even more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_values = sampler(mu_init=-1, nsamples=100000, proposal_std_dev=0.01)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mu_values)\n",
    "ax.set_xlabel('step')\n",
    "ax.set_ylabel('$\\mu$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like it is jumping around back and forth. Let's histogram these samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "out = ax.hist(mu_values, bins=100)\n",
    "ax.set_xlabel('$\\mu$')\n",
    "ax.set_ylabel('P($\\mu$| data)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like a posterior distribution and it is (sort of). We have written a (basic) [Metropolis Hasting Marcov-Chain Monte Carlo (MCMC)](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) sampler. These samplers satisfy a condition known as [detailed balance](https://en.wikipedia.org/wiki/Detailed_balance), which means that ** in the limit of taking a sufficiently large number of samples, the distribution of the samples approximates that of the distribution itself **. However, as you'll note there are several issues with this posterior! Let's go through them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auto correlation\n",
    "\n",
    "Let's take a look at some of the samples closely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "ax1.plot(mu_values[5000:5100])\n",
    "out = ax2.hist(mu_values[5000:5100], bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These don't look like indepedent draws from the posterior at all! This is because the samples are **correlated**. We can see this by looking at the estimated autocorrelation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autocorr(x):\n",
    "    result = np.correlate(x, x, mode='full')\n",
    "    maxcorr = np.argmax(result)\n",
    "    result = result / result[result.size//2]\n",
    "    return result[result.size//2:]\n",
    "\n",
    "rho = autocorr(mu_values)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(rho)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are some great notes by [Alan Sokal](http://www.stat.unc.edu/faculty/cji/Sokal.pdf) on this topic (for general Markov Chains). One take away message that you should always keep in your head when looking at MCMC output is that \n",
    "\n",
    "> ** the number of efectively independent samples in a run of length n is roughly **\n",
    "\n",
    "$$ \\frac{n}{2\\tau} = \\frac{\\textrm{Number of samples in the MCMC walk}}{2 \\times \\textrm{integrated autocorrelation\n",
    "time}} $$ \n",
    "\n",
    "The integrated autocorrelation time is defined in those notes.\n",
    "\n",
    "Estimating the autocorrelation time is not easy, luckily the authors of [`emcee`](http://dfm.io/emcee/current/) have provided a useful interface we can use (also see the nice discussion [here](http://emcee.readthedocs.io/en/latest/tutorials/autocorr/)). Here we use that function to estimate the autocorrelation time of our chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emcee\n",
    "tau = emcee.autocorr.integrated_time(np.array(mu_values), c=2)\n",
    "print(tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note - you can install `emcee` with `$ pip install emcee`.\n",
    "\n",
    "So for our samples, how many independent samples do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(mu_values) / (2 * tau)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That is not very many.. it might be tempting to just run the sampler for longer. But first, let's look at how we generated the samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposal distrtibution\n",
    "\n",
    "You'll have seen throughout the previous examples that set `proposal_std_dev=0.01`. This variable decides how new jumps are proposed. Choosing this value is a balance:\n",
    "\n",
    "* If it is **too small** then the sampler will be inefficient, taking a long time to explore the space (with many calls to the likelihood and prior), but few of these will be independent.\n",
    "* On the other hand, if it is **too large** then few proposals will be accepted - it will spend must of the time jumping away from the posterior. \n",
    "\n",
    "One metric to help with this is the *acceptance fraction*, i.e., the number or proposed jumps that where accepted. You'll notice we have been printing this after each run and it was typically around `0.9` (indicating that `proposal_std_std` is too small).\n",
    "\n",
    "Let's increase it, we ideally (for this sampler) will aim for an acceptance fraction of 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_values = sampler(mu_init=-1, nsamples=10000, proposal_std_dev=0.6)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(mu_values)\n",
    "ax.set_xlabel('step')\n",
    "ax.set_ylabel('$\\mu$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That looks betters, now calculate the autocorrelation time and number of independent samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tau = emcee.autocorr.integrated_time(np.array(mu_values), c=2)\n",
    "n_independent_samples = len(mu_values) / (2 * tau)\n",
    "print(tau, n_independent_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks much better. Note that if you want to discard correlated samples you can **thin**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thinned_samples = mu_values[::int(tau)]\n",
    "print(len(thinned_samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burn-in\n",
    "\n",
    "Firstly, MCMC-samplers can suffer from a problem known as *burn-in*. This is the effect of the initialisation of the chain (which is usually chosen arbitrarily) on the posterior. Let's exagerate this effect by using a small proposal_std_dev to make it clear:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_values = sampler(mu_init=-5, nsamples=10000, proposal_std_dev=0.05)\n",
    "fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(8, 4))\n",
    "ax1.plot(mu_values)\n",
    "ax1.set_xlabel('step')\n",
    "ax1.set_ylabel('$\\mu$')\n",
    "\n",
    "ax2.hist(mu_values, bins=30, normed=True)\n",
    "ax2.set_xlabel('$\\mu$')\n",
    "ax2.set_ylabel('$P(\\mu| data)$')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we see that there is a \"tail\" from the fact that the chain was started quite far away from the posterior. One easy way to deal with this is to remove the first few hundred steps (or more precisely, a few times the autocorrelation length, of which more later) from the chain before generating the posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note - this will show up on the plot above\n",
    "nburn = 2000\n",
    "ax1.plot(range(len(mu_values))[nburn:], mu_values[nburn:])\n",
    "ax2.hist(mu_values[nburn:], bins=30, normed=True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Okay, so let's fix all the aforementioned problems and look at the posterior:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mu_values = sampler(mu_init=-1, nsamples=50000, proposal_std_dev=0.6)  # Generate samples\n",
    "tau = int(emcee.autocorr.integrated_time(np.array(mu_values), c=2))  # Calculate tau\n",
    "mu_values = mu_values[tau:]  # Drop the first tau samples (the burn-in)\n",
    "mu_values = mu_values[::tau]  # Thin the samples by tau\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "out = ax.hist(mu_values, bins=50)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
